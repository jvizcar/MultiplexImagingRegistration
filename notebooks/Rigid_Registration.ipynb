{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigid Registration\n",
    "2020-07-28\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T18:26:03.595081Z",
     "start_time": "2020-07-29T18:26:03.555665Z"
    }
   },
   "outputs": [],
   "source": [
    "# autoreloads imported modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T19:20:21.337807Z",
     "start_time": "2020-07-29T19:20:21.287145Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import cv2\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from tqdm.notebook import tqdm\n",
    "from pandas import DataFrame, read_csv, concat\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "from modules.utils import parse_tif_dir, normalize_image\n",
    "from modules.registration import register_images_adv, tre_distance, apply_transform\n",
    "\n",
    "from os.path import join\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to set before running the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T19:57:39.819101Z",
     "start_time": "2020-07-29T19:57:39.772907Z"
    }
   },
   "outputs": [],
   "source": [
    "# set variables for run\n",
    "\n",
    "# quench must be set to False\n",
    "filepaths = parse_tif_dir('/media/Registration/Datasets/tonsils/', quench=False)\n",
    "\n",
    "# ft_extractor = cv2.AKAZE_create()\n",
    "# ft_extractor = cv2.KAZE_create(extended=True)\n",
    "ft_extractor = cv2.xfeatures2d.SURF_create(hessianThreshold=1500)\n",
    "\n",
    "matcher = cv2.BFMatcher(normType=cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# save path\n",
    "save_dir = '/mnt/RigidRegistrationFigures/tonsils_SURF_BF-Matcher'\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "target_round = 1\n",
    "channels = [1, 2]  # channels to test out\n",
    "\n",
    "data = {'target_round': target_round, 'moving_round': [], 'channel': [], 'raw_error': [], 'registered_error': [],\n",
    "        'num_kpts': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-29T19:57:41.620Z"
    }
   },
   "outputs": [],
   "source": [
    "# add this to avoid showing figures, prevents kernel dying\n",
    "%matplotlib agg\n",
    "\n",
    "max_round = max(list(filepaths.keys()))\n",
    "\n",
    "for channel in channels:\n",
    "    print(f'Running on channel {channel}')\n",
    "    \n",
    "    channel_dir = join(save_dir, f'channel_{channel}')\n",
    "    makedirs(channel_dir, exist_ok=True)\n",
    "    \n",
    "    # get target image\n",
    "    target_im = normalize_image(imread(filepaths[target_round][channel]))\n",
    "\n",
    "    # loop through the rest of the rounds\n",
    "    for _round in tqdm(range(1, max_round+1), total=len(range(1, max_round+1))):\n",
    "        # skip the target round\n",
    "        if _round != target_round: \n",
    "            # get moving image\n",
    "            moving_im = normalize_image(imread(filepaths[_round][channel]))\n",
    "\n",
    "            save_path = join(channel_dir, f'targetRound-{target_round}_movingRound-{_round}_channel-{channel}.png')\n",
    "\n",
    "            # run registration with saving image\n",
    "            results = register_images_adv(moving_im, target_im, ft_extractor, matcher, visuals=False, \n",
    "                                    savepath=save_path)\n",
    "            \n",
    "            # add results to data\n",
    "            data['moving_round'].append(_round)\n",
    "            data['channel'].append(channel)\n",
    "            data['raw_error'].append(results['error (raw)'])\n",
    "            data['registered_error'].append(results['error (registered)'])\n",
    "            data['num_kpts'].append(results['n_filtered_kpts'])\n",
    "            \n",
    "# save the data as dataframe\n",
    "df = DataFrame(data)\n",
    "df.to_csv(join(save_dir, 'results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df = DataFrame(\n",
    "    columns=['target_round', 'moving_round', 'channel', 'raw_error', 'registered_error', 'num_kpts', 'ft'])\n",
    "\n",
    "\n",
    "df1 = read_csv('/mnt/RigidRegistrationFigures/normalBreast_KAZE_BF-Matcher/results.csv')\n",
    "df1['ft'] = 'KAZE'\n",
    "df2 = read_csv('/mnt/RigidRegistrationFigures/normalBreast_AKAZE_BF-Matcher/results.csv')\n",
    "df2['ft'] = 'AKAZE'\n",
    "df3 = read_csv('/mnt/RigidRegistrationFigures/normalBreast_SURF_BF-Matcher/results.csv')\n",
    "df3['ft'] = 'SURF'\n",
    "\n",
    "df = concat([df1, df2, df3])\n",
    "\n",
    "g = sns.catplot(x='ft', y='registered_error', hue='channel', data=df, kind='swarm', height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(sns.catplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version two of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for the run\n",
    "data_dir = '/media/Registration/Datasets/normalBreast/'\n",
    "\n",
    "target_round = 1\n",
    "dapi_round = 1\n",
    "\n",
    "matcher = cv2.BFMatcher(normType=cv2.NORM_L2, crossCheck=True)\n",
    "ft_ext = ft_extractor = cv2.AKAZE_create()\n",
    "transformer = transform.SimilarityTransform()\n",
    "\n",
    "# parse the directory\n",
    "impaths = parse_tif_dir(data_dir, quench=False)\n",
    "\n",
    "# get the list of rounds\n",
    "rounds = list(impaths.keys())\n",
    "rounds.sort()\n",
    "\n",
    "# remove first index if it is zero\n",
    "rounds = rounds[1:] if rounds[0] == 0 else rounds\n",
    "\n",
    "# get target DAPI image\n",
    "target_dapi_im = normalize_image(imread(impaths[target_round][dapi_round]))\n",
    "\n",
    "# extract features from target DAPI image\n",
    "# target_dapi_kpts, target_dapi_des = ft_ext.detectAndCompute(target_dapi_im, None)\n",
    "\n",
    "data = {'target_round': target_round, 'moving_round': [], 'method': [], 'desc': [], 'error': []}\n",
    "\n",
    "# loop through the rest of the rounds\n",
    "for r in rounds:\n",
    "    if r == target_round:\n",
    "        continue\n",
    "    \n",
    "    # register DAPI channel\n",
    "    \n",
    "    # moving DAPI image and descriptors\n",
    "#     moving_dapi_im = normalize_image(imread(impaths[r][dapi_round]))\n",
    "#     moving_dapi_kpts, moving_dapi_des = ft_ext.detectAndCompute(moving_dapi_im, None)\n",
    "\n",
    "    moving_dapi_filtered_pts, target_dapi_filtered_pts = filter_keypoints(\n",
    "        moving_dapi_kpts, moving_dapi_des, target_dapi_kpts, target_dapi_des\n",
    "    )\n",
    "    \n",
    "    # calculate the unregistered error\n",
    "    h, w = target_dapi_im.shape[:2]\n",
    "    unregistered_error = tre_distance(moving_dapi_filtered_pts, target_dapi_filtered_pts, h, w)\n",
    "    \n",
    "    # add to data\n",
    "    data['moving_round'].append(r)\n",
    "    data['method'].append('unregistered')\n",
    "    data['desc'].append('Breast (AKAZE)')\n",
    "    data['error'].append(unregistered_error)\n",
    "    \n",
    "    # register the DAPI channel\n",
    "    registered_dapi_im = apply_transform(moving_dapi_im, target_dapi_im, moving_dapi_filtered_pts, \n",
    "                                         target_dapi_filtered_pts, transformer)[0]\n",
    "    registered_dapi_im = (registered_dapi_im * 255).astype(np.uint8)\n",
    "    \n",
    "    # get features\n",
    "    moving_reg_kpts, moving_reg_des = ft_ext.detectAndCompute(registered_dapi_im, None)\n",
    "    \n",
    "    moving_reg_filtered_pts, target_reg_filtered_pts = filter_keypoints(\n",
    "        moving_reg_kpts, moving_reg_des, target_dapi_kpts, target_dapi_des\n",
    "    )\n",
    "    \n",
    "    h, w = moving_dapi_im.shape[:2]\n",
    "    registered_error = tre_distance(moving_reg_filtered_pts, target_reg_filtered_pts, h, w)\n",
    "    data['moving_round'].append(r)\n",
    "    data['method'].append('registered DAPI')\n",
    "    data['desc'].append('Breast (AKAZE)')\n",
    "    data['error'].append(registered_error)\n",
    "    \n",
    "    \n",
    "    # register using second channel\n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_round': 1,\n",
       " 'moving_round': [2, 2],\n",
       " 'method': ['unregistered', 'registered DAPI'],\n",
       " 'desc': ['Breast (AKAZE)', 'Breast (AKAZE)'],\n",
       " 'error': [0.02590011, 0.00011252031]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_keypoints(moving_kpts, moving_des, target_kpts, target_des):\n",
    "    matches = matcher.match(moving_des, target_des)\n",
    "    moving_matched_kpts = [moving_kpts[match.queryIdx] for match in matches]\n",
    "    target_matched_kpts = [target_kpts[match.trainIdx] for match in matches]\n",
    "    \n",
    "    # convert matched keypoints to array\n",
    "    moving_matched_pts = np.float32([kpt.pt for kpt in moving_matched_kpts])  # (x,y) coords\n",
    "    target_matched_pts = np.float32([kpt.pt for kpt in target_matched_kpts])  # (x,y) coords\n",
    "    \n",
    "    # filter keypoints using RANSAC mask\n",
    "    mask = cv2.findHomography(\n",
    "        moving_matched_pts, target_matched_pts, cv2.RANSAC, ransacReprojThreshold=10)[1]\n",
    "    moving_filtered_kpts = [moving_matched_kpts[i] for i in np.arange(0, len(mask)) if mask[i] == [1]]\n",
    "    target_filtered_kpts = [target_matched_kpts[i] for i in np.arange(0, len(mask)) if mask[i] == [1]]\n",
    "    \n",
    "    # convert filtered keypoints to array\n",
    "    moving_filtered_pts = np.float32([kpt.pt for kpt in moving_filtered_kpts])\n",
    "    target_filtered_pts = np.float32([kpt.pt for kpt in target_filtered_kpts])\n",
    "    \n",
    "    return moving_filtered_pts, target_filtered_pts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
